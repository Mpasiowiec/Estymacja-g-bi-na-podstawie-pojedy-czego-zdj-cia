base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset/data_nyu_2.yaml
- config/model_sdv2.yaml


dataset:
  train:
    name: vkitti2
    disp_name: vkitti2_train
    dir: vkitti2
    filenames: data_split/vkitti2_train.txt
    kitti_bm_crop: true
    valid_mask_crop: null  # no valid_mask_crop for training
    norm_name: vkitti2
  val:
    - name: vkitti2
      disp_name: vkitti2_val
      dir: vkitti2
      filenames: data_split/vkitti2_val.txt
      kitti_bm_crop: true
      valid_mask_crop: eigen
      norm_name: vkitti2

model:
  name: TernausNet
  backbone: vgg16

depth_normalization:
  type: scale_shift_depth
  clip: true
  norm_min: -1.0
  norm_max: 1.0
  min_max_quantile: 0.02

augmentation_args:
  random_horizontal_flip:
    in_use: false
    p: 0.5
  jitter:
    in_use: false
    p: 0.5
    args:
      hue: 0.1
      brightness: 0.1
      contrast: 0.1
  cutdepth:
    in_use: false
    p: 0.5
    depth_type: linear
    par: 0.5
  red_green_channel_swap:
    in_use: false
    p: 0.5

dataloader:
  num_workers: 2
  effective_batch_size: 1
  max_train_batch_size: 1
  seed: 2024  # to ensure continuity when resuming from checkpoint

# Training settings
trainer:
  init_seed: 2024  # use null to train w/o seeding
  save_period: 50
  backup_period: 2000
  validation_period: 2000

gt_depth_type: depth_raw_norm
gt_mask_type: valid_mask_raw

max_epoch: 100  # a large enough number
max_iter: 30000  # usually converges at around 20k

optimizer:
  name: Adam

loss:
  name: mse_loss
  kwargs:
    reduction: mean

lr: 3.0e-05
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 25000
    final_ratio: 0.01
    warmup_steps: 100

# Validation (and visualization) settings
validation:
  main_val_metric: abs_relative_difference
  main_val_metric_goal: minimize
  init_seed: 2024

eval:
  eval_metrics:
  - abs_relative_difference
  - squared_relative_difference
  - rmse_linear
  - rmse_log
  - log10
  - delta1_acc
  - delta2_acc
  - delta3_acc
  - i_rmse
  - silog_rmse
