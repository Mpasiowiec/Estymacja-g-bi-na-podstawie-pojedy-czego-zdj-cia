{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.MiDas import MidasNet\n",
    "from models.TernausNet import UNet16 \n",
    "\n",
    "from datasets import BaseDepthDataset, DatasetMode, get_dataset\n",
    "from datasets.mixed_sampler import MixedBatchSampler\n",
    "from trainers.trainer import NetTrainer\n",
    "from util.config_util import recursive_load_config\n",
    "from util.depth_transform import (\n",
    "    DepthNormalizerBase,\n",
    "    get_depth_normalizer,\n",
    ")\n",
    "from util.logging_util import config_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = datetime.now()\n",
    "print(f\"start at {t_start}\")\n",
    "# -------------------- Arguments --------------------\n",
    "class argumenty():\n",
    "    def __init__(self) -> None:\n",
    "        self.config = './config/test_nyu_laptop.yaml'\n",
    "        self.resume_run = './test_files/24_09_04-02_27_19-test_nyu_laptop_mixloss/checkpoint/latest'\n",
    "        self.output_dir = './test_files'\n",
    "        self.no_cuda = False\n",
    "        self.exit_after = -1\n",
    "        self.base_data_dir = \"C:/Users/micha/Downloads/nyu_v2-20240826T235024Z-001\"\n",
    "        self.add_datetime_prefix = True\n",
    "        \n",
    "args = argumenty()\n",
    "resume_run = args.resume_run\n",
    "output_dir = args.output_dir\n",
    "base_data_dir = args.base_data_dir\n",
    "\n",
    "# -------------------- Initialization --------------------\n",
    "# Resume previous run\n",
    "if resume_run is not None:\n",
    "    print(f\"Resume run: {resume_run}\")\n",
    "    out_dir_run = os.path.dirname(os.path.dirname(resume_run))\n",
    "    job_name = os.path.basename(out_dir_run)\n",
    "    # Resume config file\n",
    "    cfg = OmegaConf.load(os.path.join(out_dir_run, \"config.yaml\"))\n",
    "else:\n",
    "    # Run from start\n",
    "    cfg = recursive_load_config(args.config)\n",
    "    # Full job name\n",
    "    pure_job_name = os.path.basename(args.config).split(\".\")[0]\n",
    "    # Add time prefix\n",
    "    if args.add_datetime_prefix:\n",
    "        job_name = f\"{t_start.strftime('%y_%m_%d-%H_%M_%S')}-{pure_job_name}\"\n",
    "    else:\n",
    "        job_name = pure_job_name\n",
    "\n",
    "    # Output dir\n",
    "    if output_dir is not None:\n",
    "        out_dir_run = os.path.join(output_dir, job_name)\n",
    "    else:\n",
    "        out_dir_run = os.path.join(\"./output\", job_name)\n",
    "    os.makedirs(out_dir_run, exist_ok=False)\n",
    "\n",
    "# Other directories\n",
    "out_dir_dic = {\n",
    "    'ckpt'  : os.path.join(out_dir_run, \"checkpoint\"),\n",
    "    'rec' : os.path.join(out_dir_run, \"records\"),\n",
    "}\n",
    "for key in out_dir_dic.keys():\n",
    "    if not os.path.exists(out_dir_dic[key]):\n",
    "        os.makedirs(out_dir_dic[key])\n",
    "\n",
    "# -------------------- Logging settings --------------------\n",
    "config_logging(cfg.logging, out_dir=out_dir_run)\n",
    "logging.debug(f\"config: {cfg}\")\n",
    "# -------------------- Device --------------------\n",
    "cuda_avail = torch.cuda.is_available() and not args.no_cuda\n",
    "device = torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
    "logging.info(f\"device = {device}\")\n",
    "\n",
    "# -------------------- Snapshot of code and config --------------------\n",
    "if resume_run is None:\n",
    "    _output_path = os.path.join(out_dir_run, \"config.yaml\")\n",
    "    with open(_output_path, \"w+\") as f:\n",
    "        OmegaConf.save(config=cfg, f=f)\n",
    "    logging.info(f\"Config saved to {_output_path}\")\n",
    "    # Copy and tar code on the first run\n",
    "    _temp_code_dir = os.path.join(out_dir_run, \"code_tar\")\n",
    "    _code_snapshot_path = os.path.join(out_dir_run, \"code_snapshot.tar\")\n",
    "    os.system(\n",
    "        f\"rsync --relative -arhvz --quiet --filter=':- .gitignore' --exclude '.git' . '{_temp_code_dir}'\"\n",
    "    )\n",
    "    os.system(f\"tar -cf {_code_snapshot_path} {_temp_code_dir}\")\n",
    "    os.system(f\"rm -rf {_temp_code_dir}\")\n",
    "    logging.info(f\"Code snapshot saved to: {_code_snapshot_path}\")\n",
    "\n",
    "# -------------------- Data --------------------\n",
    "loader_seed = cfg.dataloader.seed\n",
    "if loader_seed is None:\n",
    "    loader_generator = None\n",
    "else:\n",
    "    loader_generator = torch.Generator().manual_seed(loader_seed)\n",
    "\n",
    "# Training dataset\n",
    "depth_transform: DepthNormalizerBase = get_depth_normalizer(\n",
    "    cfg_normalizer=cfg.depth_normalization\n",
    ")\n",
    "train_dataset: BaseDepthDataset = get_dataset(\n",
    "    cfg.dataset.train,\n",
    "    base_data_dir=base_data_dir,\n",
    "    mode=DatasetMode.TRAIN,\n",
    "    augmentation_args=cfg.augmentation_args,\n",
    "    depth_transform=depth_transform,\n",
    "    gt_depth_type=cfg.gt_depth_type\n",
    ")\n",
    "\n",
    "if \"mixed\" == cfg.dataset.train.name:\n",
    "    dataset_ls = train_dataset\n",
    "    assert len(cfg.dataset.train.prob_ls) == len(\n",
    "        dataset_ls\n",
    "    ), \"Lengths don't match: `prob_ls` and `dataset_list`\"\n",
    "    concat_dataset = ConcatDataset(dataset_ls)\n",
    "    mixed_sampler = MixedBatchSampler(\n",
    "        src_dataset_ls=dataset_ls,\n",
    "        batch_size=cfg.dataloader.train_batch_size,\n",
    "        drop_last=True,\n",
    "        prob=cfg.dataset.train.prob_ls,\n",
    "        shuffle=True,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        concat_dataset,\n",
    "        batch_sampler=mixed_sampler,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        pin_memory=cfg.dataloader.pin_memory, \n",
    "    )\n",
    "else:\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=cfg.dataloader.train_batch_size,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        shuffle=True,\n",
    "        generator=loader_generator,\n",
    "        pin_memory=cfg.dataloader.pin_memory,\n",
    "    )\n",
    "# Validation dataset\n",
    "val_dataset: BaseDepthDataset = get_dataset(\n",
    "    cfg.dataset.val,\n",
    "    base_data_dir=base_data_dir,\n",
    "    mode=DatasetMode.EVAL,\n",
    "    depth_transform=depth_transform,\n",
    "    gt_depth_type=cfg.gt_depth_type\n",
    ")\n",
    "if \"mixed\" == cfg.dataset.val.name:\n",
    "    dataset_ls = val_dataset\n",
    "    assert len(cfg.dataset.val.prob_ls) == len(\n",
    "        dataset_ls\n",
    "    ), \"Lengths don't match: `prob_ls` and `dataset_list`\"\n",
    "    concat_dataset = ConcatDataset(dataset_ls)\n",
    "    mixed_sampler = MixedBatchSampler(\n",
    "        src_dataset_ls=dataset_ls,\n",
    "        batch_size=cfg.dataloader.val_batch_size,\n",
    "        drop_last=True,\n",
    "        prob=cfg.dataset.val.prob_ls,\n",
    "        shuffle=False,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        concat_dataset,\n",
    "        batch_sampler=mixed_sampler,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        pin_memory=cfg.dataloader.pin_memory,\n",
    "    )\n",
    "else:\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=cfg.dataloader.val_batch_size,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        shuffle=False,\n",
    "        generator=loader_generator,\n",
    "        pin_memory=cfg.dataloader.pin_memory,\n",
    "    )\n",
    "# Test dataset\n",
    "test_loaders: List[DataLoader] = []\n",
    "for _test_dic in cfg.dataset.test:\n",
    "    _test_dataset = get_dataset(\n",
    "        _test_dic,\n",
    "        base_data_dir=base_data_dir,\n",
    "        mode=DatasetMode.EVAL,\n",
    "        depth_transform=depth_transform,\n",
    "        gt_depth_type=cfg.gt_depth_type\n",
    "    )\n",
    "    _test_loader = DataLoader(\n",
    "        dataset=_test_dataset,\n",
    "        batch_size=cfg.dataloader.test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        pin_memory=cfg.dataloader.pin_memory,\n",
    "    )\n",
    "    test_loaders.append(_test_loader)\n",
    "\n",
    "dataloaders = {\n",
    "    'train' : train_loader,\n",
    "    'val' : val_loader,\n",
    "    'tests' : test_loaders,\n",
    "}\n",
    "\n",
    "# -------------------- Model --------------------\n",
    "if cfg.model.name == 'TernausNet':\n",
    "    model = UNet16(pretrained=True, is_deconv=True)\n",
    "elif cfg.model.name == 'MiDas':\n",
    "  model = MidasNet(backbone=cfg.model.backbone)\n",
    "else:\n",
    "  raise NotImplementedError \n",
    "\n",
    "# -------------------- Trainer --------------------\n",
    "# Exit time\n",
    "if args.exit_after > 0:\n",
    "    t_end = t_start + timedelta(minutes=args.exit_after)\n",
    "    logging.info(f\"Will exit at {t_end}\")\n",
    "else:\n",
    "    t_end = None\n",
    "\n",
    "logging.debug(f\"Trainer: treiner_nets\")\n",
    "trainer = NetTrainer(\n",
    "    cfg=cfg,\n",
    "    model=model,\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    out_dir_dic=out_dir_dic,\n",
    ")\n",
    "\n",
    "# -------------------- Checkpoint --------------------\n",
    "if resume_run is not None:\n",
    "    trainer.load_checkpoint(\n",
    "        resume_run, load_trainer_state=True, resume_lr_scheduler=True\n",
    "    )\n",
    "\n",
    "# -------------------- Training & Evaluation Loop --------------------\n",
    "try:\n",
    "    trainer.train_and_validate(t_end=t_end)\n",
    "except Exception as e:\n",
    "    logging.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONF / DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.config_util import recursive_load_config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "conf_path = './config/test_nyu_laptop.yaml'\n",
    "cfg = recursive_load_config(conf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetMode, get_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from util.depth_transform import (\n",
    "    DepthNormalizerBase,\n",
    "    get_depth_normalizer,\n",
    ")\n",
    "\n",
    "depth_transform: DepthNormalizerBase = get_depth_normalizer(\n",
    "    cfg_normalizer=cfg.depth_normalization\n",
    ")\n",
    "\n",
    "test_loaders = []\n",
    "for _test_dic in cfg.dataset.test:\n",
    "    _test_dataset = get_dataset(\n",
    "        _test_dic,\n",
    "        base_data_dir='C:/Users/micha/Downloads/nyu_v2-20240826T235024Z-001',\n",
    "        mode=DatasetMode.EVAL,\n",
    "        gt_depth_type=cfg.gt_depth_type,\n",
    "        depth_transform=depth_transform,\n",
    "    )\n",
    "    _test_loader = DataLoader(\n",
    "        dataset=_test_dataset,\n",
    "        batch_size=cfg.dataloader.test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        pin_memory=cfg.dataloader.pin_memory,\n",
    "    )\n",
    "    test_loaders.append(_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Model --------------------\n",
    "if cfg.model.name == 'TernausNet':\n",
    "    model = UNet16(pretrained=True, is_deconv=True)\n",
    "elif cfg.model.name == 'MiDas':\n",
    "  model = MidasNet(backbone=cfg.model.backbone)\n",
    "else:\n",
    "  raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path = glob.glob('./test_files/*'+conf_path.split('/')[-1].split('.')[-2]+'/checkpoint/latest/best_net.pth')\n",
    "model.load_state_dict(\n",
    "    torch.load(path[0], map_location=device)\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path = glob.glob('./test_files/*'+conf_path.split('/')[-1].split('.')[-2]+'/checkpoint/latest/net.pth')\n",
    "model.load_state_dict(\n",
    "    torch.load(path[0], map_location=device)\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIZUALIZACJA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best output comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from util.config_util import recursive_load_config\n",
    "from models.MiDas import MidasNet\n",
    "from models.TernausNet import UNet16\n",
    "from util.alignment import align_depth_least_square\n",
    "\n",
    "\n",
    "def imgshow(inp, title=None, pred=False):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    if not pred:\n",
    "        mean = np.array([0.48012177, 0.41071795, 0.39187136])\n",
    "        std = np.array([0.28875302, 0.29516797, 0.30792887])\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "        plt.imshow(inp)\n",
    "    else:\n",
    "        plt.imshow(inp, cmap='viridis')\n",
    "    plt.axis(\"off\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "config_list = glob.glob('./config/*test_nyu_laptop_lin*.yaml')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i, conf in enumerate(config_list):\n",
    "    cfg = recursive_load_config(conf)\n",
    "    \n",
    "    # -------------------- Model --------------------\n",
    "    if cfg.model.name == 'TernausNet':\n",
    "        model = UNet16(pretrained=True, is_deconv=True)\n",
    "    elif cfg.model.name == 'MiDas':\n",
    "      model = MidasNet(backbone=cfg.model.backbone)\n",
    "    else:\n",
    "      raise NotImplementedError\n",
    "    \n",
    "    best_path = glob.glob('./test_files/*'+conf.split('\\\\')[-1].split('.')[-2]+'/checkpoint/latest/best_net.pth')\n",
    "    model.load_state_dict(\n",
    "        torch.load(best_path[0], map_location=device)\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    batch = next(iter(test_loaders[0]))\n",
    "    \n",
    "    if i == 0:\n",
    "        fig, (a1,a2) = plt.subplots(1,2)\n",
    "        a1.imshow(batch['rgb_img'][0][0])\n",
    "        a2.imshow(batch['depth_raw_linear'][0][0])\n",
    "        \n",
    "    pred = model(batch['rgb_img'][0].unsqueeze(0).to(device))\n",
    "    pred_a = align_depth_least_square(batch['depth_raw_linear'][0].unsqueeze(0), pred.detach().cpu(), batch['valid_mask_raw'][0].unsqueeze(0), return_scale_shift=False)\n",
    "\n",
    "    fig, (a1,a2) = plt.subplots(1,2)\n",
    "    a1.imshow(pred.detach().cpu().numpy()[0][0])\n",
    "    a1.title.set_text('pred_'+conf.split('\\\\')[-1].split('.')[-2])\n",
    "    a2.imshow(pred_a[0][0])\n",
    "    a2.title.set_text('pred_alig_'+conf.split('\\\\')[-1].split('.')[-2])\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### record comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "tr_list = glob.glob('./test_files/*test_nyu_laptop*/*/train*.csv')\n",
    "vl_list = glob.glob('./test_files/*test_nyu_laptop*/*/val*.csv')\n",
    "\n",
    "\n",
    "tr0 = pd.read_csv(tr_list[0])\n",
    "\n",
    "\n",
    "for name in tr0.columns[1:]:\n",
    "    f, (a1,a2) = plt.subplots(1,2)\n",
    "    f.suptitle(name)\n",
    "    for i in range(len(tr_list)):\n",
    "        tr = pd.read_csv(tr_list[i])\n",
    "        vl = pd.read_csv(vl_list[i])\n",
    "        a1.plot(tr['epoch'], tr[name], label = tr_list[i].split('\\\\')[-3].split('_')[7:])\n",
    "        a2.plot(vl['epoch'], vl[name], label = vl_list[i].split('\\\\')[-3].split('_')[7:])\n",
    "        a1.legend()\n",
    "        a2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TINCKERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
