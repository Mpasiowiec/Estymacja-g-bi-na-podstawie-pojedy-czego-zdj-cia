{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.MiDas import MidasNet\n",
    "from models.TernausNet import UNet16 \n",
    "\n",
    "from datasets import BaseDepthDataset, DatasetMode, get_dataset\n",
    "from datasets.mixed_sampler import MixedBatchSampler\n",
    "from trainers.mytrainer import NetTrainer\n",
    "from util.config_util import (\n",
    "    find_value_in_omegaconf,\n",
    "    recursive_load_config,\n",
    ")\n",
    "from util.depth_transform import (\n",
    "    DepthNormalizerBase,\n",
    "    get_depth_normalizer,\n",
    ")\n",
    "from util.logging_util import(\n",
    "  config_logging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self) -> None:\n",
    "        self.config = \"config/test_rel.yaml\"\n",
    "        self.resume_run = None # './test/checkpoint/latest'\n",
    "        self.output_dir = \"./\" # \"/content/drive/MyDrive/magisterka\"\n",
    "        self.base_data_dir = 'C:/Users/micha/Downloads/nyu_v2-20240826T235024Z-001'\n",
    "        self.add_datetime_prefix = False\n",
    "        self.exit_after = -1\n",
    "        self.no_cuda = False\n",
    "\n",
    "args = args()\n",
    "\n",
    "resume_run = args.resume_run\n",
    "output_dir = args.output_dir\n",
    "base_data_dir = (\n",
    "    args.base_data_dir\n",
    "    if args.base_data_dir is not None\n",
    "    else os.environ[\"BASE_DATA_DIR\"]\n",
    ")\n",
    "t_start = datetime.now()\n",
    "# Resume previous run\n",
    "if resume_run is not None:\n",
    "    print(f\"Resume run: {resume_run}\")\n",
    "    out_dir_run = os.path.dirname(os.path.dirname(resume_run))\n",
    "    job_name = os.path.basename(out_dir_run)\n",
    "    # Resume config file\n",
    "    cfg = OmegaConf.load(os.path.join(out_dir_run, \"config.yaml\"))\n",
    "else:\n",
    "    # Run from start\n",
    "    cfg = recursive_load_config(args.config)\n",
    "    # Full job name\n",
    "    pure_job_name = os.path.basename(args.config).split(\".\")[0]\n",
    "    # Add time prefix\n",
    "    if args.add_datetime_prefix:\n",
    "        job_name = f\"{t_start.strftime('%y_%m_%d-%H_%M_%S')}-{pure_job_name}\"\n",
    "    else:\n",
    "        job_name = pure_job_name\n",
    "    # Output dir\n",
    "    if output_dir is not None:\n",
    "        out_dir_run = os.path.join(output_dir, job_name)\n",
    "    else:\n",
    "        out_dir_run = os.path.join(\"./output\", job_name)\n",
    "    os.makedirs(out_dir_run, exist_ok=False)\n",
    "cfg_data = cfg.dataset\n",
    "# Other directories\n",
    "out_dir_ckpt = os.path.join(out_dir_run, \"checkpoint\")\n",
    "if not os.path.exists(out_dir_ckpt):\n",
    "    os.makedirs(out_dir_ckpt)\n",
    "out_dir_tr = os.path.join(out_dir_run, \"training_record\")\n",
    "if not os.path.exists(out_dir_tr):\n",
    "    os.makedirs(out_dir_tr)\n",
    "out_dir_eval = os.path.join(out_dir_run, \"evaluation_record\")\n",
    "if not os.path.exists(out_dir_eval):\n",
    "    os.makedirs(out_dir_eval)\n",
    "# -------------------- Logging settings --------------------\n",
    "config_logging(cfg.logging, out_dir=out_dir_run)\n",
    "logging.debug(f\"config: {cfg}\")\n",
    "# -------------------- Device --------------------\n",
    "cuda_avail = torch.cuda.is_available() and not args.no_cuda\n",
    "device = torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
    "logging.info(f\"device = {device}\")\n",
    "# -------------------- Snapshot of code and config --------------------\n",
    "if resume_run is None:\n",
    "    _output_path = os.path.join(out_dir_run, \"config.yaml\")\n",
    "    with open(_output_path, \"w+\") as f:\n",
    "        OmegaConf.save(config=cfg, f=f)\n",
    "    logging.info(f\"Config saved to {_output_path}\")\n",
    "    # Copy and tar code on the first run\n",
    "    _temp_code_dir = os.path.join(out_dir_run, \"code_tar\")\n",
    "    _code_snapshot_path = os.path.join(out_dir_run, \"code_snapshot.tar\")\n",
    "    os.system(\n",
    "        f\"rsync --relative -arhvz --quiet --filter=':- .gitignore' --exclude '.git' . '{_temp_code_dir}'\"\n",
    "    )\n",
    "    os.system(f\"tar -cf {_code_snapshot_path} {_temp_code_dir}\")\n",
    "    os.system(f\"rm -rf {_temp_code_dir}\")\n",
    "    logging.info(f\"Code snapshot saved to: {_code_snapshot_path}\")\n",
    "# -------------------- Data --------------------\n",
    "loader_seed = cfg.dataloader.seed\n",
    "if loader_seed is None:\n",
    "    loader_generator = None\n",
    "else:\n",
    "    loader_generator = torch.Generator().manual_seed(loader_seed)\n",
    "\n",
    "# Training dataset\n",
    "depth_transform: DepthNormalizerBase = get_depth_normalizer(\n",
    "    cfg_normalizer=cfg.depth_normalization\n",
    ")\n",
    "train_dataset: BaseDepthDataset = get_dataset(\n",
    "    cfg_data.train,\n",
    "    base_data_dir=base_data_dir,\n",
    "    mode=DatasetMode.TRAIN,\n",
    "    augmentation_args=cfg.augmentation_args,\n",
    "    depth_transform=depth_transform,\n",
    ")\n",
    "logging.debug(\"Augmentation: \", cfg.augmentation_args)\n",
    "if \"mixed\" == cfg_data.train.name:\n",
    "    dataset_ls = train_dataset\n",
    "    assert len(cfg_data.train.prob_ls) == len(\n",
    "        dataset_ls\n",
    "    ), \"Lengths don't match: `prob_ls` and `dataset_list`\"\n",
    "    concat_dataset = ConcatDataset(dataset_ls)\n",
    "    mixed_sampler = MixedBatchSampler(\n",
    "        src_dataset_ls=dataset_ls,\n",
    "        batch_size=cfg.dataloader.train_batch_size,\n",
    "        drop_last=True,\n",
    "        prob=cfg_data.train.prob_ls,\n",
    "        shuffle=True,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        concat_dataset,\n",
    "        batch_sampler=mixed_sampler,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "    )\n",
    "else:\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=cfg.dataloader.train_batch_size,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        shuffle=True,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "# Validation dataset\n",
    "val_dataset: BaseDepthDataset = get_dataset(\n",
    "    cfg_data.val,\n",
    "    base_data_dir=base_data_dir,\n",
    "    mode=DatasetMode.TRAIN,\n",
    "    depth_transform=depth_transform,\n",
    ")\n",
    "if \"mixed\" == cfg_data.val.name:\n",
    "    dataset_ls = val_dataset\n",
    "    assert len(cfg_data.val.prob_ls) == len(\n",
    "        dataset_ls\n",
    "    ), \"Lengths don't match: `prob_ls` and `dataset_list`\"\n",
    "    concat_dataset = ConcatDataset(dataset_ls)\n",
    "    mixed_sampler = MixedBatchSampler(\n",
    "        src_dataset_ls=dataset_ls,\n",
    "        batch_size=cfg.dataloader.val_batch_size,\n",
    "        drop_last=True,\n",
    "        prob=cfg_data.val.prob_ls,\n",
    "        shuffle=False,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        concat_dataset,\n",
    "        batch_sampler=mixed_sampler,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "    )\n",
    "else:\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=cfg.dataloader.val_batch_size,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        shuffle=False,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "# Test dataset\n",
    "test_loaders: List[DataLoader] = []\n",
    "for _test_dic in cfg_data.test:\n",
    "    _test_dataset = get_dataset(\n",
    "        _test_dic,\n",
    "        base_data_dir=base_data_dir,\n",
    "        mode=DatasetMode.TRAIN,\n",
    "        depth_transform=depth_transform,\n",
    "    )\n",
    "    _test_loader = DataLoader(\n",
    "        dataset=_test_dataset,\n",
    "        batch_size=cfg.dataloader.test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "    )\n",
    "    test_loaders.append(_test_loader)\n",
    "# -------------------- Model --------------------\n",
    "if cfg.model.name == 'TernausNet':\n",
    "    model = UNet16(pretrained=True, is_deconv=True)\n",
    "elif cfg.model.name == 'MiDas':\n",
    "  model = MidasNet(backbone=cfg.model.backbone)\n",
    "else:\n",
    "  raise NotImplementedError\n",
    "# -------------------- Trainer --------------------\n",
    "# Exit time\n",
    "if args.exit_after > 0:\n",
    "    t_end = t_start + timedelta(minutes=args.exit_after)\n",
    "    logging.info(f\"Will exit at {t_end}\")\n",
    "else:\n",
    "    t_end = None\n",
    "logging.debug(f\"Trainer: treiner_nets\")\n",
    "trainer = NetTrainer(\n",
    "    cfg=cfg,\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    device=device,\n",
    "    out_dir_ckpt=out_dir_ckpt,\n",
    "    out_dir_tr=out_dir_tr,\n",
    "    out_dir_eval=out_dir_eval,\n",
    "    val_dataloader=val_loader,\n",
    "    test_dataloaders=test_loaders,\n",
    ")\n",
    "# -------------------- Checkpoint --------------------\n",
    "if resume_run is not None:\n",
    "    trainer.load_checkpoint(\n",
    "        resume_run, load_trainer_state=True, resume_lr_scheduler=True\n",
    "    )\n",
    "# -------------------- Training & Evaluation Loop --------------------\n",
    "try:\n",
    "    trainer.train(t_end=t_end)\n",
    "except Exception as e:\n",
    "    logging.exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "def imgshow(inp, title=None, pred=False):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    if not pred:\n",
    "        mean = np.array([0.48012177, 0.41071795, 0.39187136])\n",
    "        std = np.array([0.28875302, 0.29516797, 0.30792887])\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "# for samples, _ in iter(dataloaders['train']):\n",
    "batch = next(iter(train_loader))\n",
    "plt.figure(figsize=(16,24))\n",
    "grid_imgs = torchvision.utils.make_grid(batch['rgb_img'])\n",
    "imgshow(grid_imgs)\n",
    "\n",
    "pred_samples = model(batch['rgb_img'].to(device))\n",
    "\n",
    "plt.figure(figsize=(16,24))\n",
    "grid_imgs = torchvision.utils.make_grid(pred_samples.cpu())\n",
    "imgshow(grid_imgs, pred=True)\n",
    "\n",
    "plt.figure(figsize=(16,24))\n",
    "grid_imgs = torchvision.utils.make_grid(batch['depth_raw_norm'])\n",
    "imgshow(grid_imgs, pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self) -> None:\n",
    "        self.config = \"config/test_abs.yaml\"\n",
    "        self.resume_run = None # './test_abs/checkpoint/latest'\n",
    "        self.output_dir = \"./\" # \"/content/drive/MyDrive/magisterka\"\n",
    "        self.base_data_dir = 'C:/Users/micha/Downloads/nyu_v2-20240826T235024Z-001'\n",
    "        self.add_datetime_prefix = False\n",
    "        self.exit_after = -1\n",
    "        self.no_cuda = False\n",
    "\n",
    "args = args()\n",
    "\n",
    "resume_run = args.resume_run\n",
    "output_dir = args.output_dir\n",
    "base_data_dir = (\n",
    "    args.base_data_dir\n",
    "    if args.base_data_dir is not None\n",
    "    else os.environ[\"BASE_DATA_DIR\"]\n",
    ")\n",
    "t_start = datetime.now()\n",
    "# Resume previous run\n",
    "if resume_run is not None:\n",
    "    print(f\"Resume run: {resume_run}\")\n",
    "    out_dir_run = os.path.dirname(os.path.dirname(resume_run))\n",
    "    job_name = os.path.basename(out_dir_run)\n",
    "    # Resume config file\n",
    "    cfg = OmegaConf.load(os.path.join(out_dir_run, \"config.yaml\"))\n",
    "else:\n",
    "    # Run from start\n",
    "    cfg = recursive_load_config(args.config)\n",
    "    # Full job name\n",
    "    pure_job_name = os.path.basename(args.config).split(\".\")[0]\n",
    "    # Add time prefix\n",
    "    if args.add_datetime_prefix:\n",
    "        job_name = f\"{t_start.strftime('%y_%m_%d-%H_%M_%S')}-{pure_job_name}\"\n",
    "    else:\n",
    "        job_name = pure_job_name\n",
    "    # Output dir\n",
    "    if output_dir is not None:\n",
    "        out_dir_run = os.path.join(output_dir, job_name)\n",
    "    else:\n",
    "        out_dir_run = os.path.join(\"./output\", job_name)\n",
    "    os.makedirs(out_dir_run, exist_ok=False)\n",
    "cfg_data = cfg.dataset\n",
    "# Other directories\n",
    "out_dir_ckpt = os.path.join(out_dir_run, \"checkpoint\")\n",
    "if not os.path.exists(out_dir_ckpt):\n",
    "    os.makedirs(out_dir_ckpt)\n",
    "out_dir_tr = os.path.join(out_dir_run, \"training_record\")\n",
    "if not os.path.exists(out_dir_tr):\n",
    "    os.makedirs(out_dir_tr)\n",
    "out_dir_eval = os.path.join(out_dir_run, \"evaluation_record\")\n",
    "if not os.path.exists(out_dir_eval):\n",
    "    os.makedirs(out_dir_eval)\n",
    "# -------------------- Logging settings --------------------\n",
    "config_logging(cfg.logging, out_dir=out_dir_run)\n",
    "logging.debug(f\"config: {cfg}\")\n",
    "# -------------------- Device --------------------\n",
    "cuda_avail = torch.cuda.is_availabel() and not args.no_cuda\n",
    "device = torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
    "logging.info(f\"device = {device}\")\n",
    "# -------------------- Snapshot of code and config --------------------\n",
    "if resume_run is None:\n",
    "    _output_path = os.path.join(out_dir_run, \"config.yaml\")\n",
    "    with open(_output_path, \"w+\") as f:\n",
    "        OmegaConf.save(config=cfg, f=f)\n",
    "    logging.info(f\"Config saved to {_output_path}\")\n",
    "    # Copy and tar code on the first run\n",
    "    _temp_code_dir = os.path.join(out_dir_run, \"code_tar\")\n",
    "    _code_snapshot_path = os.path.join(out_dir_run, \"code_snapshot.tar\")\n",
    "    os.system(\n",
    "        f\"rsync --relative -arhvz --quiet --filter=':- .gitignore' --exclude '.git' . '{_temp_code_dir}'\"\n",
    "    )\n",
    "    os.system(f\"tar -cf {_code_snapshot_path} {_temp_code_dir}\")\n",
    "    os.system(f\"rm -rf {_temp_code_dir}\")\n",
    "    logging.info(f\"Code snapshot saved to: {_code_snapshot_path}\")\n",
    "# -------------------- Data --------------------\n",
    "loader_seed = cfg.dataloader.seed\n",
    "if loader_seed is None:\n",
    "    loader_generator = None\n",
    "else:\n",
    "    loader_generator = torch.Generator().manual_seed(loader_seed)\n",
    "# Training dataset\n",
    "depth_transform: DepthNormalizerBase = get_depth_normalizer(\n",
    "    cfg_normalizer=cfg.depth_normalization\n",
    ")\n",
    "train_dataset: BaseDepthDataset = get_dataset(\n",
    "    cfg_data.train,\n",
    "    base_data_dir=base_data_dir,\n",
    "    mode=DatasetMode.TRAIN,\n",
    "    augmentation_args=cfg.augmentation_args,\n",
    "    depth_transform=depth_transform,\n",
    ")\n",
    "logging.debug(\"Augmentation: \", cfg.augmentation_args)\n",
    "if \"mixed\" == cfg_data.train.name:\n",
    "    dataset_ls = train_dataset\n",
    "    assert len(cfg_data.train.prob_ls) == len(\n",
    "        dataset_ls\n",
    "    ), \"Lengths don't match: `prob_ls` and `dataset_list`\"\n",
    "    concat_dataset = ConcatDataset(dataset_ls)\n",
    "    mixed_sampler = MixedBatchSampler(\n",
    "        src_dataset_ls=dataset_ls,\n",
    "        batch_size=cfg.dataloader.train_batch_size,\n",
    "        drop_last=True,\n",
    "        prob=cfg_data.train.prob_ls,\n",
    "        shuffle=True,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        concat_dataset,\n",
    "        batch_sampler=mixed_sampler,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "    )\n",
    "else:\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=cfg.dataloader.train_batch_size,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        shuffle=True,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "# Validation dataset\n",
    "val_dataset: BaseDepthDataset = get_dataset(\n",
    "    cfg_data.val,\n",
    "    base_data_dir=base_data_dir,\n",
    "    mode=DatasetMode.TRAIN,\n",
    "    depth_transform=depth_transform,\n",
    ")\n",
    "if \"mixed\" == cfg_data.val.name:\n",
    "    dataset_ls = val_dataset\n",
    "    assert len(cfg_data.val.prob_ls) == len(\n",
    "        dataset_ls\n",
    "    ), \"Lengths don't match: `prob_ls` and `dataset_list`\"\n",
    "    concat_dataset = ConcatDataset(dataset_ls)\n",
    "    mixed_sampler = MixedBatchSampler(\n",
    "        src_dataset_ls=dataset_ls,\n",
    "        batch_size=cfg.dataloader.val_batch_size,\n",
    "        drop_last=True,\n",
    "        prob=cfg_data.val.prob_ls,\n",
    "        shuffle=False,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        concat_dataset,\n",
    "        batch_sampler=mixed_sampler,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "    )\n",
    "else:\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=cfg.dataloader.val_batch_size,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "        shuffle=False,\n",
    "        generator=loader_generator,\n",
    "    )\n",
    "# Test dataset\n",
    "test_loaders: List[DataLoader] = []\n",
    "for _test_dic in cfg_data.test:\n",
    "    _test_dataset = get_dataset(\n",
    "        _test_dic,\n",
    "        base_data_dir=base_data_dir,\n",
    "        mode=DatasetMode.TRAIN,\n",
    "        depth_transform=depth_transform,\n",
    "    )\n",
    "    _test_loader = DataLoader(\n",
    "        dataset=_test_dataset,\n",
    "        batch_size=cfg.dataloader.test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataloader.num_workers,\n",
    "    )\n",
    "    test_loaders.append(_test_loader)\n",
    "# -------------------- Model --------------------\n",
    "if cfg.model.name == 'TernausNet':\n",
    "    model = UNet16(pretrained=True, is_deconv=True)\n",
    "elif cfg.model.name == 'MiDas':\n",
    "  model = MidasNet(backbone=cfg.model.backbone)\n",
    "else:\n",
    "  raise NotImplementedError\n",
    "# -------------------- Trainer --------------------\n",
    "# Exit time\n",
    "if args.exit_after > 0:\n",
    "    t_end = t_start + timedelta(minutes=args.exit_after)\n",
    "    logging.info(f\"Will exit at {t_end}\")\n",
    "else:\n",
    "    t_end = None\n",
    "logging.debug(f\"Trainer: treiner_nets\")\n",
    "trainer = NetTrainer(\n",
    "    cfg=cfg,\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    device=device,\n",
    "    out_dir_ckpt=out_dir_ckpt,\n",
    "    out_dir_tr=out_dir_tr,\n",
    "    out_dir_eval=out_dir_eval,\n",
    "    val_dataloader=val_loader,\n",
    "    test_dataloaders=test_loaders,\n",
    ")\n",
    "# -------------------- Checkpoint --------------------\n",
    "if resume_run is not None:\n",
    "    trainer.load_checkpoint(\n",
    "        resume_run, load_trainer_state=True, resume_lr_scheduler=True\n",
    "    )\n",
    "# -------------------- Training & Evaluation Loop --------------------\n",
    "try:\n",
    "    trainer.train(t_end=t_end)\n",
    "except Exception as e:\n",
    "    logging.exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "plt.figure(figsize=(16,24))\n",
    "grid_imgs = torchvision.utils.make_grid(batch['rgb_img'])\n",
    "imgshow(grid_imgs)\n",
    "\n",
    "pred_samples = model(batch['rgb_img'].to(device))\n",
    "\n",
    "plt.figure(figsize=(16,24))\n",
    "grid_imgs = torchvision.utils.make_grid(pred_samples.cpu())\n",
    "imgshow(grid_imgs, pred=True)\n",
    "\n",
    "plt.figure(figsize=(16,24))\n",
    "grid_imgs = torchvision.utils.make_grid(batch['depth_raw_norm'])\n",
    "imgshow(grid_imgs, pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "tr_abs = pd.read_csv('./test_abs/training_record/train_record.csv')\n",
    "tr_rel = pd.read_csv('./test_rel/training_record/train_record.csv')\n",
    "val_abs = pd.read_csv('./test_abs/evaluation_record/eval_record.csv')\n",
    "val_rel = pd.read_csv('./test_rel/evaluation_record/eval_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for metric_name in tr_abs.columns[1:]:\n",
    "    fig, (a1,a2) = plt.subplots(1,2)\n",
    "    fig.suptitle(metric_name)\n",
    "    a1.plot(tr_abs['epoch'], tr_abs[metric_name], label = 'tr')\n",
    "    a1.plot(val_abs['epoch'], val_abs[metric_name], label = 'val')\n",
    "    a1.legend()\n",
    "    \n",
    "    a2.plot(tr_rel['epoch'], tr_rel[metric_name], label = 'tr')\n",
    "    a2.plot(val_rel['epoch'], val_rel[metric_name], label = 'val')\n",
    "    a2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 3.4\n",
      "1.33 1.375\n",
      "[544, 160] 87040\n",
      "[352, 256] 90112\n",
      "[256, 256] 65536\n"
     ]
    }
   ],
   "source": [
    "a = pow(2,5)\n",
    "a1=17\n",
    "a2=5\n",
    "b1=11\n",
    "b2=8\n",
    "print(3.45, a1/a2)\n",
    "print(1.33, b1/b2)\n",
    "print([a * a1, a * a2], a * a1 * a * a2)\n",
    "print([a * b1, a * b2], a * b1 * a * b2)\n",
    "print([256,256], 256*256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
